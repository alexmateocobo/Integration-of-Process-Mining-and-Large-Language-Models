{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "This script uses libraries for cloud data access (google.cloud.bigquery), process mining and visualization (pm4py), environment management (dotenv), data handling (pandas), and language model orchestration (langchain and langchain_openai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for cloud data access\n",
    "from google.cloud import bigquery\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for logging\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data handling\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for process mining\n",
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for RAG (Retrieval-Augmented Generation)\n",
    "from pinecone import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries for loading environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication Setup\n",
    "\n",
    "Before accessing Google Cloud services, a service account must be created in the Google Cloud Console, with the necessary IAM roles (e.g., BigQuery Admin) assigned to it. The service account’s JSON key file is securely stored locally, and its path is set using the GOOGLE_APPLICATION_CREDENTIALS environment variable to enable programmatic authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 11:20:19,738 - INFO - Starting OAuth authentication...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=316641064865-57id3o26obibotvs226jeevisjdujha5.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A51350%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform&state=8Tey3xSSvnwo7WprOFtv0TARLz4LLN&access_type=offline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 11:20:25,781 - INFO - \"GET /?state=8Tey3xSSvnwo7WprOFtv0TARLz4LLN&code=4/0AVMBsJhSGxVbFmpp_ElFyf7G7dbsHlgK4JTq4S6BPwXslj4ZAwdCM9TQ3lZtSnvAy1Veqw&scope=https://www.googleapis.com/auth/cloud-platform HTTP/1.1\" 200 65\n",
      "2025-07-12 11:20:26,145 - INFO - Authentication successful.\n",
      "2025-07-12 11:20:26,146 - INFO - BigQuery client initialized.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "project_id = \"integration-of-pm-and-llms\"\n",
    "client_secret_path = \"/Users/alejandromateocobo/Documents/PythonProjects/Integration_Of_LLMs_And_Process_Mining/keys/client_secret_316641064865-57id3o26obibotvs226jeevisjdujha5.apps.googleusercontent.com.json\"\n",
    "\n",
    "# Authentication flow\n",
    "logging.info(\"Starting OAuth authentication...\")\n",
    "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\n",
    "    client_secret_path,\n",
    "    scopes=SCOPES\n",
    ")\n",
    "credentials = flow.run_local_server(port=0)\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "logging.info(\"Authentication successful.\")\n",
    "logging.info(\"BigQuery client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query BigQuery\n",
    "\n",
    "Google Cloud Platform (GCP) is a suite of cloud computing services that enables scalable storage, processing, and data analysis using Google’s infrastructure. To use GCP for analyzing clinical datasets like MIMIC-III, users must create a Google account, set up a GCP project with billing, enable the necessary APIs, and configure OAuth client authentication to securely access cloud resources. The MIMIC-III database, which contains detailed health records from over 40,000 critical care patients, can be accessed through Google BigQuery for efficient cloud-based analysis, which is the recommended method by the MIT Lab for Computational Physiology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 12:03:45,959 - INFO - Running user-provided query...\n",
      "2025-07-12 12:03:47,160 - INFO - Waiting for query to complete...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID 9da185b4-94aa-4bcd-bc0f-9ec8e1b66e7f successfully executed: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 12:03:48,928 - INFO - Query completed. Retrieved 1616 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT e.*\n",
    "        FROM `integration-of-pm-and-llms.integration_of_pm_and_llms.filtered_eventlog` e\n",
    "        INNER JOIN `physionet-data.mimiciii_clinical.icustays` icu\n",
    "        ON e.icustay_id = icu.icustay_id\n",
    "        WHERE e.icustay_id IN (211555, 290738, 236225, 213113)\n",
    "        AND e.linksto = 'datetimeevents'\n",
    "    \"\"\"\n",
    "\n",
    "# Check if the client is initialized before running the query\n",
    "if not client:\n",
    "    raise Exception(\"BigQuery client not initialized. Run authenticate() first.\")\n",
    "\n",
    "# Run the query\n",
    "logging.info(\"Running user-provided query...\")\n",
    "job = client.query(query)\n",
    "\n",
    "logging.info(\"Waiting for query to complete...\")\n",
    "df = job.to_dataframe(progress_bar_type='tqdm')\n",
    "\n",
    "logging.info(f\"Query completed. Retrieved {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   itemid  subject_id  hadm_id  icustay_id     event_timestamp  \\\n",
      "0  224295       47546   112012      236225 2113-04-18 18:09:00   \n",
      "1  224282       97599   135263      213113 2142-01-11 00:00:00   \n",
      "2  225766       97599   135263      213113 2142-01-01 00:00:00   \n",
      "3  224279       97599   135263      213113 2142-01-20 06:00:00   \n",
      "4  224290       97599   135263      213113 2142-01-17 16:45:00   \n",
      "\n",
      "                               label                 category         linksto  \n",
      "0  Cordis/Introducer Dressing Change  Access Lines - Invasive  datetimeevents  \n",
      "1          Multi Lumen Tubing Change  Access Lines - Invasive  datetimeevents  \n",
      "2              Sheath Insertion Date  Access Lines - Invasive  datetimeevents  \n",
      "3        Multi Lumen Dressing Change  Access Lines - Invasive  datetimeevents  \n",
      "4        Arterial line Tubing Change  Access Lines - Invasive  datetimeevents  \n"
     ]
    }
   ],
   "source": [
    "# Process the dataframe as needed\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Event Log for PM4PY\n",
    "\n",
    "In this step, the dataset is reformatted to match the structure expected by PM4PY, where each event log requires a case identifier, an activity name, and a timestamp. The data is then converted into a PM4PY event log object, which enables process mining algorithms to analyze the sequence of events across cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 12:03:57,004 - INFO - Total cases: 4\n",
      "2025-07-12 12:03:57,009 - INFO - Total activities: 35\n",
      "2025-07-12 12:03:57,018 - INFO - Total events: 1616\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for PM4PY (using icustay_id as case identifier)\n",
    "df_eventlog = df.rename(columns={\n",
    "    \"icustay_id\": \"case:concept:name\",  # Update to use icustay_id as case identifier\n",
    "    \"event_timestamp\": \"time:timestamp\",\n",
    "    \"label\": \"concept:name\"\n",
    "})\n",
    "\n",
    "# Convert the timestamp column to datetime\n",
    "df_eventlog[\"time:timestamp\"] = pd.to_datetime(df_eventlog[\"time:timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Format the dataframe using pm4py.utils.format_dataframe\n",
    "df_eventlog = pm4py.utils.format_dataframe(\n",
    "    df_eventlog,\n",
    "    case_id='case:concept:name',\n",
    "    activity_key='concept:name',\n",
    "    timestamp_key='time:timestamp'\n",
    ")\n",
    "\n",
    "# Convert the dataframe to an event log object\n",
    "event_log = pm4py.convert_to_event_log(df_eventlog)\n",
    "\n",
    "# Print basic statistics\n",
    "logging.info(f\"Total cases: {len(set(df_eventlog['case:concept:name']))}\")\n",
    "logging.info(f\"Total activities: {len(set(df_eventlog['concept:name']))}\")\n",
    "logging.info(f\"Total events: {len(df_eventlog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   itemid  subject_id  hadm_id case:concept:name            time:timestamp  \\\n",
      "0  226515       53232   115492            211555 2128-01-02 00:00:00+00:00   \n",
      "1  224290       53232   115492            211555 2151-06-29 00:00:00+00:00   \n",
      "2  224298       53232   115492            211555 2151-06-29 00:00:00+00:00   \n",
      "3  224288       53232   115492            211555 2151-06-29 00:00:00+00:00   \n",
      "4  224298       53232   115492            211555 2151-06-29 00:00:00+00:00   \n",
      "\n",
      "                      concept:name                 category         linksto  \\\n",
      "0                    Date of Birth                      ADT  datetimeevents   \n",
      "1      Arterial line Tubing Change  Access Lines - Invasive  datetimeevents   \n",
      "2  Cordis/Introducer Tubing Change  Access Lines - Invasive  datetimeevents   \n",
      "3     Arterial line Insertion Date  Access Lines - Invasive  datetimeevents   \n",
      "4  Cordis/Introducer Tubing Change  Access Lines - Invasive  datetimeevents   \n",
      "\n",
      "   @@index  @@case_index  \n",
      "0        0             0  \n",
      "1        1             0  \n",
      "2        2             0  \n",
      "3        3             0  \n",
      "4        4             0  \n"
     ]
    }
   ],
   "source": [
    "# Preview the formatted event log\n",
    "print(df_eventlog.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discover and Visualize the Directly-Follows Graph (DFG) with PM4PY\n",
    "\n",
    "In this step, the frequency-based Directly-Follows Graph (DFG) is discovered from the event log using PM4PY and visualized to understand the control-flow structure of the process. The DFG is then converted into a readable textual format to prepare it for further analysis with a Large Language Model (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover the Directly-Follows Graph (DFG)\n",
    "dfg, start_activities, end_activities = pm4py.discovery.discover_dfg(\n",
    "    event_log,\n",
    "    activity_key='concept:name',\n",
    "    case_id_key='case:concept:name',\n",
    "    timestamp_key='time:timestamp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the DFG\n",
    "pm4py.vis.view_dfg(\n",
    "    dfg,\n",
    "    start_activities,\n",
    "    end_activities,\n",
    "    format='png',       # You can also use 'svg' if supported\n",
    "    bgcolor='white',\n",
    "    rankdir='LR'        # Left-to-right layout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover Petri net using the Inductive Miner\n",
    "net, im, fm = pm4py.discovery.discover_petri_net_inductive(\n",
    "    event_log,\n",
    "    activity_key='concept:name',\n",
    "    case_id_key='case:concept:name',\n",
    "    timestamp_key='time:timestamp',\n",
    "    multi_processing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Petri net\n",
    "pm4py.vis.view_petri_net(net, im, fm, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pinecone and OpenAI keys\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(name=\"integration-of-pm-and-llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from generated folder\n",
    "loader = DirectoryLoader(\n",
    "    \"/Users/alejandromateocobo/Documents/PythonProjects/Integration_Of_LLMs_And_Process_Mining/data/context\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 13:33:38,579 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-12 13:33:40,353 - INFO - Documents successfully embedded and stored in Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Split text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunked_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Generate unique chunk IDs\n",
    "ids = [f\"doc_{i}\" for i in range(len(chunked_docs))]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "\n",
    "# Store documents in Pinecone\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "vector_store.add_documents(documents=chunked_docs, ids=ids)\n",
    "\n",
    "logging.info(\"Documents successfully embedded and stored in Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 13:40:04,621 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and knowledgeable assistant specialized in process mining. Your role is to analyze process-related data, models, and documentation to provide accurate and concise answers.\n",
      "\n",
      "Here’s a user question: What is the most common workflow for patients in the ICU??\n",
      "\n",
      "Here is some extracted documentation and context:\n",
      "\n",
      "Abstract\n",
      "MIMIC-III is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. The database includes information such as demographics, vital sign measurements made at the bedside (~1 data point per hour), laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality (including post-hospital discharge). MIMIC supports a diverse range of analytic studies spanning epidemiology, clinical decision-rule improvement, and electronic tool development. It is notable for three factors: it is freely available to researchers worldwide; it encompasses a diverse and very large population of ICU patients; and it contains highly granular data, including vital signs, laboratory results, and medications.\n",
      "\n",
      "O2 Flow → PH (dipstick) (frequency = 4)\n",
      "Specific Gravity (urine) → Tidal Volume (observed) (frequency = 4)\n",
      "Tidal Volume (observed) → Tidal Volume (observed) (frequency = 133)\n",
      "Tidal Volume (observed) → PSV Level (frequency = 3)\n",
      "PSV Level → PSV Level (frequency = 75)\n",
      "PSV Level → Admission Weight (lbs.) (frequency = 3)\n",
      "Admission Weight (lbs.) → Admission Weight (lbs.) (frequency = 14)\n",
      "Admission Weight (lbs.) → Impaired Skin Length #1 (frequency = 3)\n",
      "Impaired Skin Length #1 → Impaired Skin Length #1 (frequency = 139)\n",
      "Impaired Skin Length #1 → Impaired Skin Depth #1 (frequency = 2)\n",
      "Impaired Skin Depth #1 → Impaired Skin Depth #1 (frequency = 136)\n",
      "Impaired Skin Depth #1 → Impaired Skin Width #1 (frequency = 2)\n",
      "Impaired Skin Width #1 → Impaired Skin Width #1 (frequency = 139)\n",
      "Impaired Skin Width #1 → Ventilator Tank #1 (frequency = 3)\n",
      "Ventilator Tank #1 → Ventilator Tank #1 (frequency = 63)\n",
      "Ventilator Tank #1 → Ventilator Tank #2 (frequency = 4)\n",
      "\n",
      "MIMIC-III integrates deidentified, comprehensive clinical data of patients admitted to the Beth Israel Deaconess Medical Center in Boston, Massachusetts, and makes it widely accessible to researchers internationally under a data use agreement. The open nature of the data allows clinical studies to be reproduced and improved in ways that would not otherwise be possible. The MIMIC-III database was populated with data that had been acquired during routine hospital care, so there was no associated burden on caregivers and no interference with their workflow. Data was downloaded from several sources, including: archives from critical care information systems. hospital electronic health record databases. Social Security Administration Death Master File. Two different critical care information systems were in place over the data collection period: Philips CareVue Clinical Information System (models M2331A and M1215A; Philips Health-care, Andover, MA) and iMDsoft MetaVision ICU (iMDsoft,\n",
      "\n",
      "Needham, MA). These systems were the source of clinical data such as: time-stamped nurse-verified physiological measurements (for example, hourly documentation of heart rate, arterial blood pressure, or respiratory rate); documented progress notes by care providers; continuous intravenous drip medications and fluid balances. With exception to data relating to fluid intake, which differed significantly in structure between the CareVue and MetaVision systems, data was merged when building the database tables. Data which could not be merged is given a suffix to denote the data source. For example, inputs for patients monitored with the CareVue system are stored in INPUTEVENTS_CV, whereas inputs for patients monitored with the Metavision system are stored in INPUTEVENTS_MV. Additional information was collected from hospital and laboratory health record systems, including: patient demographics and in-hospital mortality. laboratory test results (for example, hematology, chemistry, and\n",
      "\n",
      "Ventilator Tank #2 → Ventilator Tank #2 (frequency = 63)\n",
      "Ventilator Tank #2 → Glucose finger stick (frequency = 4)\n",
      "Glucose finger stick → Glucose finger stick (frequency = 94)\n",
      "Glucose finger stick → MDI #1 Puff (frequency = 2)\n",
      "MDI #1 Puff → MDI #1 Puff (frequency = 54)\n",
      "MDI #1 Puff → MDI #2 Puff (frequency = 1)\n",
      "MDI #2 Puff → MDI #2 Puff (frequency = 7)\n",
      "MDI #2 Puff → Cuff Pressure (frequency = 1)\n",
      "Cuff Pressure → Cuff Pressure (frequency = 17)\n",
      "Cuff Pressure → Spont Vt (frequency = 3)\n",
      "Spont Vt → Spont Vt (frequency = 6)\n",
      "Spont Vt → Spont RR (frequency = 3)\n",
      "Spont RR → Spont RR (frequency = 6)\n",
      "Spont RR → Differential-Basos (frequency = 3)\n",
      "Differential-Basos → Differential-Basos (frequency = 18)\n",
      "Differential-Basos → Differential-Eos (frequency = 5)\n",
      "Differential-Eos → Differential-Eos (frequency = 18)\n",
      "Differential-Eos → Differential-Lymphs (frequency = 5)\n",
      "Differential-Lymphs → Differential-Lymphs (frequency = 18)\n",
      "Differential-Lymphs → Differential-Monos (frequency = 5)\n",
      "\n",
      "Specific Gravity (urine) → Glucose finger stick (frequency = 1)\n",
      "Arterial O2 Saturation → Arterial O2 Saturation (frequency = 4)\n",
      "D-Dimer → D-Dimer (frequency = 2)\n",
      "MDI #1 Puff → Cuff Pressure (frequency = 1)\n",
      "HCO3 (serum) → Albumin (frequency = 1)\n",
      "TCO2 (calc) Arterial → PH (dipstick) (frequency = 1)\n",
      "Admission Weight (lbs.) → Ventilator Tank #1 (frequency = 1)\n",
      "Glucose finger stick → Cuff Pressure (frequency = 1)\n",
      "\n",
      "Potassium (serum) → HCO3 (serum) (frequency = 8)\n",
      "HCO3 (serum) → HCO3 (serum) (frequency = 62)\n",
      "HCO3 (serum) → CK-MB (frequency = 5)\n",
      "CK-MB → Albumin (frequency = 3)\n",
      "Albumin → Platelet Count (frequency = 4)\n",
      "Platelet Count → Platelet Count (frequency = 59)\n",
      "Platelet Count → Prothrombin time (frequency = 7)\n",
      "Prothrombin time → Prothrombin time (frequency = 29)\n",
      "Prothrombin time → PTT (frequency = 7)\n",
      "PTT → PTT (frequency = 32)\n",
      "PTT → INR (frequency = 7)\n",
      "INR → INR (frequency = 29)\n",
      "INR → Minute Volume Alarm - Low (frequency = 5)\n",
      "Minute Volume Alarm - Low → Minute Volume Alarm - High (frequency = 5)\n",
      "Minute Volume Alarm - High → PEEP set (frequency = 5)\n",
      "PEEP set → PEEP set (frequency = 154)\n",
      "PEEP set → Inspired O2 Fraction (frequency = 5)\n",
      "Inspired O2 Fraction → Inspired O2 Fraction (frequency = 164)\n",
      "Inspired O2 Fraction → Paw High (frequency = 5)\n",
      "Paw High → Vti High (frequency = 5)\n",
      "Vti High → Fspn High (frequency = 5)\n",
      "Fspn High → Apnea Interval (frequency = 5)\n",
      "\n",
      "involved balancing simplicity of interpretation against closeness to ground truth. As such, the model is a reflection of underlying data sources, modified over iterations of the MIMIC database in response to user feedback. Care has been taken to avoid making assumptions about the underlying data when carrying out transformations, so MIMIC-III closely represents the raw hospital data. Broadly speaking, five tables are used to define and track patient stays: ADMISSIONS; PATIENTS; ICUSTAYS; SERVICES; and TRANSFERS. Another five tables are dictionaries for cross-referencing codes against their respective definitions: D_CPT; D_ICD_DIAGNOSES; D_ICD_PROCEDURES; D_ITEMS; and D_LABITEMS. The remaining tables contain data associated with patient care, such as physiological measurements, caregiver observations, and billing information. In some cases it would be possible to merge tables—for example, the D_ICD_PROCEDURES and CPTEVENTS tables both contain detail relating to procedures and could be\n",
      "\n",
      "microbiology results). discharge summaries and reports of electrocardiogram and imaging studies. billing-related information such as International Classification of Disease, 9th Edition (ICD-9) codes, Diagnosis Related Group (DRG) codes, and Current Procedural Terminology (CPT) codes. Out-of-hospital mortality dates were obtained using the Social Security Administration Death Master File. Before data was incorporated into the MIMIC-III database, it was first deidentified in accordance with Health Insurance Portability and Accountability Act (HIPAA) standards using structured data cleansing and date shifting. The deidentification process for structured data required the removal of all eighteen of the identifying data elements listed in HIPAA, including fields such as patient name, telephone number, address, and dates. In particular, dates were shifted into the future by a random offset for each individual patient in a consistent manner to preserve intervals, resulting in stays which\n",
      "\n",
      "Apnea Interval → Tidal Volume (spontaneous) (frequency = 4)\n",
      "Tidal Volume (spontaneous) → Minute Volume (frequency = 4)\n",
      "Minute Volume → Respiratory Rate (spontaneous) (frequency = 5)\n",
      "Respiratory Rate (spontaneous) → Mean Airway Pressure (frequency = 5)\n",
      "Mean Airway Pressure → Arterial O2 pressure (frequency = 5)\n",
      "Arterial O2 pressure → Arterial O2 pressure (frequency = 71)\n",
      "Arterial O2 pressure → Arterial O2 Saturation (frequency = 2)\n",
      "Arterial O2 Saturation → Arterial CO2 Pressure (frequency = 2)\n",
      "Arterial CO2 Pressure → Arterial CO2 Pressure (frequency = 71)\n",
      "Arterial CO2 Pressure → PH (Arterial) (frequency = 6)\n",
      "PH (Arterial) → PH (Arterial) (frequency = 71)\n",
      "PH (Arterial) → Arterial Base Excess (frequency = 6)\n",
      "Arterial Base Excess → Arterial Base Excess (frequency = 71)\n",
      "Arterial Base Excess → Lactic Acid (frequency = 4)\n",
      "Lactic Acid → TCO2 (calc) Arterial (frequency = 4)\n",
      "TCO2 (calc) Arterial → TCO2 (calc) Arterial (frequency = 71)\n",
      "TCO2 (calc) Arterial → O2 Flow (frequency = 5)\n",
      "\n",
      "Now please answer the question as clearly and precisely as possible:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 13:40:10,046 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"The most common workflow for patients in the ICU can be summarized through a series of frequent measures and interventions based on the data extracted from the MIMIC-III database. The workflow typically includes the following key components:\\n\\n1. **Monitoring Vital Signs**: Continuous monitoring of vital signs such as heart rate, blood pressure, respiratory rate, and oxygen saturation. Measurements are often taken at least hourly.\\n\\n2. **Ventilator Management**: Many ICU patients require mechanical ventilation. This involves:\\n   - Regular adjustments of ventilator settings (e.g., Tidal Volume, PEEP, Inspired O2 Fraction).\\n   - Continual observation of patient response (e.g., Arterial CO2 and O2 levels).\\n   \\n3. **Laboratory Testing**: Frequent laboratory tests are conducted to monitor vital biochemical parameters, such as:\\n   - Serum electrolytes (Potassium, HCO3).\\n   - Blood gas analyses (Arterial pH, Arterial Base Excess).\\n   - Other critical values (e.g., Glucose levels).\\n\\n4. **Medication Administration**: Patients receive various intravenous medications and treatments based on their condition. This includes careful management of fluid balances and monitoring of medication effects.\\n\\n5. **Skin Assessment**: Regular assessments of skin integrity are performed to prevent complications such as pressure ulcers, involving evaluations of skin length, depth, and width.\\n\\n6. **Progress Documentation**: Continuous documentation by care providers, including notes on patient progress, response to treatment, and any changes in condition.\\n\\n7. **Multidisciplinary Care**: Involvement of various healthcare specialists who may provide evaluations and intervention within the ICU setting, driving collaborative care.\\n\\nThis workflow is characterized by repeated assessments, adjustments, and interventions that focus on stabilizing the patient's condition and addressing their critical health needs. The MIMIC-III database supports this understanding by providing detailed records of these common procedures and interactions within the ICU environment.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 387, 'prompt_tokens': 2347, 'total_tokens': 2734, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-BsSsHV4z5oAFIjY2e9jJ4rHhdYHa5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f8459620-7e49-4102-af75-74176945c08d-0' usage_metadata={'input_tokens': 2347, 'output_tokens': 387, 'total_tokens': 2734, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "query = \"What is the most common workflow for patients in the ICU??\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(query, k=10)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "prompt = f\"\"\"You are a helpful and knowledgeable assistant specialized in process mining. Your role is to analyze process-related data, models, and documentation to provide accurate and concise answers.\n",
    "\n",
    "Here’s a user question: {query}\n",
    "\n",
    "Here is some extracted documentation and context:\n",
    "\n",
    "{docs_content}\n",
    "\n",
    "Now please answer the question as clearly and precisely as possible:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n",
    "answer = llm.invoke(prompt)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
