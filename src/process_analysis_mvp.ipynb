{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC: Integration of Proce Mining aand LLMs\n",
    "\n",
    "This notebook connects to Google BigQuery to extract clinical event data, applies PM4PY to discover the Directly-Follows Graph (DFG), visualizes it, and uses LangChain with GPT to analyze and summarize the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "This script uses libraries for cloud data access (google.cloud.bigquery), process mining and visualization (pm4py), environment management (dotenv), data handling (pandas), and language model orchestration (langchain and langchain_openai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for cloud data access\n",
    "from google.cloud import bigquery\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for process mining and visualization\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "from pm4py.visualization.dfg import visualizer as dfg_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for environment management\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data handling\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for large language model orchestration\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication Setup\n",
    "\n",
    "Before accessing Google Cloud services, a service account must be created in the Google Cloud Console, with the necessary IAM roles (e.g., BigQuery Admin) assigned to it. The service account’s JSON key file is securely stored locally, and its path is set using the GOOGLE_APPLICATION_CREDENTIALS environment variable to enable programmatic authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BigQuery access scope\n",
    "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "\n",
    "# Run OAuth flow in the browser\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\n",
    "    '/Users/alejandromateocobo/Documents/PythonProjects/Integration_Of_LLMs_And_Process_Mining/keys/client_secret_316641064865-57id3o26obibotvs226jeevisjdujha5.apps.googleusercontent.com.json',  # Path to the OAuth client JSON you downloaded\n",
    "    scopes=SCOPES\n",
    ")\n",
    "\n",
    "# Launch browser-based login\n",
    "credentials = flow.run_local_server(port=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query BigQuery\n",
    "\n",
    "Google Cloud Platform (GCP) is a suite of cloud computing services that enables scalable storage, processing, and data analysis using Google’s infrastructure. To use GCP for analyzing clinical datasets like MIMIC-III, users must create a Google account, set up a GCP project with billing, enable the necessary APIs, and configure OAuth client authentication to securely access cloud resources. The MIMIC-III database, which contains detailed health records from over 40,000 critical care patients, can be accessed through Google BigQuery for efficient cloud-based analysis, which is the recommended method by the MIT Lab for Computational Physiology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BigQuery client with OAuth credentials\n",
    "client = bigquery.Client(credentials=credentials, project=\"integration-of-pm-and-llms\")\n",
    "\n",
    "# Query MIMIC-III Clinical Dataset\n",
    "query = \"\"\"\n",
    "    SELECT subject_id, hadm_id, admittime, dischtime, admission_type\n",
    "    FROM `physionet-data.mimiciii_clinical.admissions`\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Event Log Creation\n",
    "\n",
    "For the purpose of this Minimum Viable Product (MVP), the MIMIC-III clinical demo dataset was downloaded in CSV format, and the event log was created using Power Query in Excel by combining multiple event tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Power Query Script</summary>\n",
    "\n",
    "```powerquery\n",
    "let\n",
    "    // Load all tables\n",
    "    chartevents = Excel.CurrentWorkbook(){[Name=\"chartevents\"]}[Content],\n",
    "    datetimeevents = Excel.CurrentWorkbook(){[Name=\"datetimeevents\"]}[Content],\n",
    "    inputevents_mv = Excel.CurrentWorkbook(){[Name=\"inputevents_mv\"]}[Content],\n",
    "    outputevents = Excel.CurrentWorkbook(){[Name=\"outputevents\"]}[Content],\n",
    "    procedureevents_mv = Excel.CurrentWorkbook(){[Name=\"procedureevents_mv\"]}[Content],\n",
    "    microbiologyevents = Excel.CurrentWorkbook(){[Name=\"microbiologyevents\"]}[Content],\n",
    "    d_items = Excel.CurrentWorkbook(){[Name=\"d_items\"]}[Content],\n",
    "\n",
    "    // ---- CHARTEVENTS ----\n",
    "    chartevents_renamed = Table.RenameColumns(chartevents, {{\"charttime\", \"event_timestamp\"}}),\n",
    "    chartevents_merged = Table.NestedJoin(chartevents_renamed, {\"itemid\"}, d_items, {\"itemid\"}, \"d_items\", JoinKind.LeftOuter),\n",
    "    chartevents_expanded = Table.ExpandTableColumn(chartevents_merged, \"d_items\", {\"label\", \"linksto\"}, {\"label\", \"linksto\"}),\n",
    "    chartevents_selected = Table.SelectColumns(chartevents_expanded, {\"itemid\", \"subject_id\", \"hadm_id\", \"icustay_id\", \"event_timestamp\", \"label\", \"linksto\"}),\n",
    "\n",
    "    // ---- DATETIMEEVENTS ----\n",
    "    datetimeevents_renamed = Table.RenameColumns(datetimeevents, {{\"value\", \"event_timestamp\"}}),\n",
    "    datetimeevents_merged = Table.NestedJoin(datetimeevents_renamed, {\"itemid\"}, d_items, {\"itemid\"}, \"d_items\", JoinKind.LeftOuter),\n",
    "    datetimeevents_expanded = Table.ExpandTableColumn(datetimeevents_merged, \"d_items\", {\"label\", \"linksto\"}, {\"label\", \"linksto\"}),\n",
    "    datetimeevents_selected = Table.SelectColumns(datetimeevents_expanded, {\"itemid\", \"subject_id\", \"hadm_id\", \"icustay_id\", \"event_timestamp\", \"label\", \"linksto\"}),\n",
    "\n",
    "    // ---- INPUTEVENTS_MV ----\n",
    "    inputevents_renamed = Table.RenameColumns(inputevents_mv, {{\"starttime\", \"event_timestamp\"}}),\n",
    "    inputevents_merged = Table.NestedJoin(inputevents_renamed, {\"itemid\"}, d_items, {\"itemid\"}, \"d_items\", JoinKind.LeftOuter),\n",
    "    inputevents_expanded = Table.ExpandTableColumn(inputevents_merged, \"d_items\", {\"label\", \"linksto\"}, {\"label\", \"linksto\"}),\n",
    "    inputevents_selected = Table.SelectColumns(inputevents_expanded, {\"itemid\", \"subject_id\", \"hadm_id\", \"icustay_id\", \"event_timestamp\", \"label\", \"linksto\"}),\n",
    "\n",
    "    // ---- OUTPUTEVENTS ----\n",
    "    outputevents_renamed = Table.RenameColumns(outputevents, {{\"charttime\", \"event_timestamp\"}}),\n",
    "    outputevents_merged = Table.NestedJoin(outputevents_renamed, {\"itemid\"}, d_items, {\"itemid\"}, \"d_items\", JoinKind.LeftOuter),\n",
    "    outputevents_expanded = Table.ExpandTableColumn(outputevents_merged, \"d_items\", {\"label\", \"linksto\"}, {\"label\", \"linksto\"}),\n",
    "    outputevents_selected = Table.SelectColumns(outputevents_expanded, {\"itemid\", \"subject_id\", \"hadm_id\", \"icustay_id\", \"event_timestamp\", \"label\", \"linksto\"}),\n",
    "\n",
    "    // ---- PROCEDUREEVENTS_MV ----\n",
    "    procedureevents_renamed = Table.RenameColumns(procedureevents_mv, {{\"starttime\", \"event_timestamp\"}}),\n",
    "    procedureevents_merged = Table.NestedJoin(procedureevents_renamed, {\"itemid\"}, d_items, {\"itemid\"}, \"d_items\", JoinKind.LeftOuter),\n",
    "    procedureevents_expanded = Table.ExpandTableColumn(procedureevents_merged, \"d_items\", {\"label\", \"linksto\"}, {\"label\", \"linksto\"}),\n",
    "    procedureevents_selected = Table.SelectColumns(procedureevents_expanded, {\"itemid\", \"subject_id\", \"hadm_id\", \"icustay_id\", \"event_timestamp\", \"label\", \"linksto\"}),\n",
    "\n",
    "    // ---- MICROBIOLOGYEVENTS (3 joins on different itemid fields) ----\n",
    "    microbio_renamed = Table.RenameColumns(microbiologyevents, {{\"charttime\", \"event_timestamp\"}}),\n",
    "\n",
    "    spec_join = Table.NestedJoin(microbio_renamed, {\"spec_itemid\"}, d_items, {\"itemid\"}, \"spec_info\", JoinKind.LeftOuter),\n",
    "    spec_expanded = Table.ExpandTableColumn(spec_join, \"spec_info\", {\"label\", \"linksto\"}, {\"label\", \"linksto\"}),\n",
    "    spec_selected = Table.SelectColumns(spec_expanded, {\"spec_itemid\", \"subject_id\", \"hadm_id\", \"event_timestamp\", \"label\", \"linksto\"}),\n",
    "    spec_renamed = Table.RenameColumns(spec_selected, {{\"spec_itemid\", \"itemid\"}}),\n",
    "\n",
    "    org_join = Table.NestedJoin(microbio_renamed, {\"org_itemid\"}, d_items, {\"itemid\"}, \"org_info\", JoinKind.LeftOuter),\n",
    "    org_expanded = Table.ExpandTableColumn(org_join, \"org_info\", {\"label\", \"linksto\"}, {\"label\", \"linksto\"}),\n",
    "    org_selected = Table.SelectColumns(org_expanded, {\"org_itemid\", \"subject_id\", \"hadm_id\", \"event_timestamp\", \"label\", \"linksto\"}),\n",
    "    org_renamed = Table.RenameColumns(org_selected, {{\"org_itemid\", \"itemid\"}}),\n",
    "\n",
    "    ab_join = Table.NestedJoin(microbio_renamed, {\"ab_itemid\"}, d_items, {\"itemid\"}, \"ab_info\", JoinKind.LeftOuter),\n",
    "    ab_expanded = Table.ExpandTableColumn(ab_join, \"ab_info\", {\"label\", \"linksto\"}, {\"label\", \"linksto\"}),\n",
    "    ab_selected = Table.SelectColumns(ab_expanded, {\"ab_itemid\", \"subject_id\", \"hadm_id\", \"event_timestamp\", \"label\", \"linksto\"}),\n",
    "    ab_renamed = Table.RenameColumns(ab_selected, {{\"ab_itemid\", \"itemid\"}}),\n",
    "\n",
    "    // ---- Combine all enriched tables ----\n",
    "    combined = Table.Combine({\n",
    "        chartevents_selected,\n",
    "        datetimeevents_selected,\n",
    "        inputevents_selected,\n",
    "        outputevents_selected,\n",
    "        procedureevents_selected,\n",
    "        spec_renamed,\n",
    "        org_renamed,\n",
    "        ab_renamed\n",
    "    }),\n",
    "\n",
    "    // Detect data type\n",
    "    ChangedcolumnType = Table.TransformColumnTypes(combined, {{\"itemid\", Int64.Type}, {\"subject_id\", Int64.Type}, {\"hadm_id\", Int64.Type}, {\"icustay_id\", Int64.Type}, {\"event_timestamp\", type datetime}, {\"label\", type text}, {\"linksto\", type text}}), \n",
    "\n",
    "    // Keep only first 10000 rows\n",
    "    limited = Table.FirstN(ChangedcolumnType, 10000)\n",
    "in\n",
    "    limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the event log CSV into a Pandas DataFrame\n",
    "eventlog_path = \"/Users/alejandromateocobo/Documents/PythonProjects/Integration_Of_LLMs_And_Process_Mining/data/mimic-iii-clinical-database-demo-1.4/c_EVENTLOG.csv\"\n",
    "df_eventlog = pd.read_csv(eventlog_path, sep=\";\")\n",
    "\n",
    "# Show the first 5 rows of the event log\n",
    "df_eventlog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Event Log for PM4PY\n",
    "\n",
    "In this step, the dataset is reformatted to match the structure expected by PM4PY, where each event log requires a case identifier, an activity name, and a timestamp. The data is then converted into a PM4PY event log object, which enables process mining algorithms to analyze the sequence of events across cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for PM4PY\n",
    "df_eventlog = df_eventlog.rename(columns={\n",
    "    \"hadm_id\": \"case:concept:name\",\n",
    "    \"event_timestamp\": \"time:timestamp\",\n",
    "    \"label\": \"concept:name\"\n",
    "})\n",
    "\n",
    "# Convert the timestamp column to datetime\n",
    "df_eventlog[\"time:timestamp\"] = pd.to_datetime(df_eventlog[\"time:timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Use PM4PY utility to ensure the dataframe is correctly formatted\n",
    "df_eventlog = dataframe_utils.convert_timestamp_columns_in_df(df_eventlog)\n",
    "\n",
    "# Convert the dataframe to an event log object\n",
    "event_log = log_converter.apply(df_eventlog)\n",
    "\n",
    "# Print basic statistics\n",
    "print(f\"Total cases: {len(set(df_eventlog['case:concept:name']))}\")\n",
    "print(f\"Total events: {len(df_eventlog)}\")\n",
    "\n",
    "# Preview the formatted event log\n",
    "df_eventlog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discover and Visualize the Directly-Follows Graph (DFG) with PM4PY\n",
    "\n",
    "In this step, the frequency-based Directly-Follows Graph (DFG) is discovered from the event log using PM4PY and visualized to understand the control-flow structure of the process. The DFG is then converted into a readable textual format to prepare it for further analysis with a Large Language Model (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover the frequency-based DFG from the event log\n",
    "dfg = dfg_discovery.apply(event_log)\n",
    "\n",
    "# Visualize the DFG\n",
    "gviz = dfg_vis.apply(dfg, variant=dfg_vis.Variants.FREQUENCY)\n",
    "dfg_vis.view(gviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_textual_dfg(dfg):\n",
    "    lines = []\n",
    "    for (a, b), freq in dfg.items():\n",
    "        line = f\"{a} → {b} (frequency = {freq})\"\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Convert the DFG to text\n",
    "dfg_text = create_textual_dfg(dfg)\n",
    "\n",
    "# Save it as a .txt file\n",
    "with open(\"/Users/alejandromateocobo/Documents/PythonProjects/Integration_Of_LLMs_And_Process_Mining/data/context/dfg_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(dfg_text)\n",
    "\n",
    "# Print the DFG in textual format\n",
    "print(dfg_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Send the DFG to ChatGPT via OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=pinecone_api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GPT model\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "# conversation_buf = ConversationChain(\n",
    "#     llm=llm,\n",
    "#     memory=ConversationBufferMemory()\n",
    "# )\n",
    "\n",
    "# conversation = ConversationChain(llm=llm)\n",
    "\n",
    "# Provide dataset description\n",
    "dataset_description = \"\"\"\n",
    "Abstract\n",
    "MIMIC-III is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. The database includes information such as demographics, vital sign measurements made at the bedside (~1 data point per hour), laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality (including post-hospital discharge). MIMIC supports a diverse range of analytic studies spanning epidemiology, clinical decision-rule improvement, and electronic tool development. It is notable for three factors: it is freely available to researchers worldwide; it encompasses a diverse and very large population of ICU patients; and it contains highly granular data, including vital signs, laboratory results, and medications.\n",
    "\n",
    "Background\n",
    "In recent years there has been a concerted move towards the adoption of digital health record systems in hospitals. In the US, for example, the number of non-federal acute care hospitals with basic digital systems increased from 9.4 to 75.5% over the 7 year period between 2008 and 2014 [1]. Despite this advance, interoperability of digital systems remains an open issue, leading to challenges in data integration. As a result, the potential that hospital data offers in terms of understanding and improving care is yet to be fully realized. In parallel, the scientific research community is increasingly coming under criticism for the lack of reproducibility of studies [2].\n",
    "\n",
    "Methods\n",
    "MIMIC-III integrates deidentified, comprehensive clinical data of patients admitted to the Beth Israel Deaconess Medical Center in Boston, Massachusetts, and makes it widely accessible to researchers internationally under a data use agreement. The open nature of the data allows clinical studies to be reproduced and improved in ways that would not otherwise be possible. The MIMIC-III database was populated with data that had been acquired during routine hospital care, so there was no associated burden on caregivers and no interference with their workflow. Data was downloaded from several sources, including: archives from critical care information systems. hospital electronic health record databases. Social Security Administration Death Master File. Two different critical care information systems were in place over the data collection period: Philips CareVue Clinical Information System (models M2331A and M1215A; Philips Health-care, Andover, MA) and iMDsoft MetaVision ICU (iMDsoft, Needham, MA). These systems were the source of clinical data such as: time-stamped nurse-verified physiological measurements (for example, hourly documentation of heart rate, arterial blood pressure, or respiratory rate); documented progress notes by care providers; continuous intravenous drip medications and fluid balances. With exception to data relating to fluid intake, which differed significantly in structure between the CareVue and MetaVision systems, data was merged when building the database tables. Data which could not be merged is given a suffix to denote the data source. For example, inputs for patients monitored with the CareVue system are stored in INPUTEVENTS_CV, whereas inputs for patients monitored with the Metavision system are stored in INPUTEVENTS_MV. Additional information was collected from hospital and laboratory health record systems, including: patient demographics and in-hospital mortality. laboratory test results (for example, hematology, chemistry, and microbiology results). discharge summaries and reports of electrocardiogram and imaging studies. billing-related information such as International Classification of Disease, 9th Edition (ICD-9) codes, Diagnosis Related Group (DRG) codes, and Current Procedural Terminology (CPT) codes. Out-of-hospital mortality dates were obtained using the Social Security Administration Death Master File. Before data was incorporated into the MIMIC-III database, it was first deidentified in accordance with Health Insurance Portability and Accountability Act (HIPAA) standards using structured data cleansing and date shifting. The deidentification process for structured data required the removal of all eighteen of the identifying data elements listed in HIPAA, including fields such as patient name, telephone number, address, and dates. In particular, dates were shifted into the future by a random offset for each individual patient in a consistent manner to preserve intervals, resulting in stays which occur sometime between the years 2100 and 2200. Time of day, day of the week, and approximate seasonality were conserved during date shifting. Dates of birth for patients aged over 89 were shifted to obscure their true age and comply with HIPAA regulations: these patients appear in the database with ages of over 300 years. Protected health information was removed from free text fields, such as diagnostic reports and physician notes, using a rigorously evaluated deidentification system based on extensive dictionary look-ups and pattern-matching with regular expressions. The components of this deidentification system are continually expanded as new data is acquired. The project was approved by the Institutional Review Boards of Beth Israel Deaconess Medical Center (Boston, MA) and the Massachusetts Institute of Technology (Cambridge, MA). Requirement for individual patient consent was waived because the project did not impact clinical care and all protected health information was deidentified.\n",
    "\n",
    "Data Description\n",
    "MIMIC-III is a relational database consisting of 26 tables. Tables are linked by identifiers which usually have the suffix ‘ID’. For example, SUBJECT_ID refers to a unique patient, HADM_ID refers to a unique admission to the hospital, and ICUSTAY_ID refers to a unique admission to an intensive care unit. Charted events such as notes, laboratory tests, and fluid balance are stored in a series of ‘events’ tables. For example the OUTPUTEVENTS table contains all measurements related to output for a given patient, while the LABEVENTS table contains laboratory test results for a patient. Tables prefixed with ‘D_’ are dictionary tables and provide definitions for identifiers. For example, every row of CHARTEVENTS is associated with a single ITEMID which represents the concept measured, but it does not contain the actual name of the measurement. By joining CHARTEVENTS and D_ITEMS on ITEMID, it is possible to identify the concept represented by a given ITEMID. Developing the MIMIC data model involved balancing simplicity of interpretation against closeness to ground truth. As such, the model is a reflection of underlying data sources, modified over iterations of the MIMIC database in response to user feedback. Care has been taken to avoid making assumptions about the underlying data when carrying out transformations, so MIMIC-III closely represents the raw hospital data. Broadly speaking, five tables are used to define and track patient stays: ADMISSIONS; PATIENTS; ICUSTAYS; SERVICES; and TRANSFERS. Another five tables are dictionaries for cross-referencing codes against their respective definitions: D_CPT; D_ICD_DIAGNOSES; D_ICD_PROCEDURES; D_ITEMS; and D_LABITEMS. The remaining tables contain data associated with patient care, such as physiological measurements, caregiver observations, and billing information. In some cases it would be possible to merge tables—for example, the D_ICD_PROCEDURES and CPTEVENTS tables both contain detail relating to procedures and could be combined—but our approach is to keep the tables independent for clarity, since the data sources are significantly different. Rather than combining the tables within MIMIC data model, we suggest researchers develop database views and transforms as appropriate.\n",
    "\"\"\"\n",
    "\n",
    "# Save it as a .txt file\n",
    "with open(\"/Users/alejandromateocobo/Documents/PythonProjects/Integration_Of_LLMs_And_Process_Mining/data/context/dataset_description.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(dataset_description)\n",
    "\n",
    "# Send the dataset description as initial context to the chain\n",
    "# conversation_buf(dataset_description)\n",
    "\n",
    "# print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Send the DFG as additional context (memory will retain the dataset description)\n",
    "conversation_buf(dfg_text)\n",
    "\n",
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Follow-up question using memory context (context is no longer needed here)\n",
    "conversation_buf(\"Which activities seem to be the most frequent?\")\n",
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Prompt\n",
    "\n",
    "pcsk_5GcY8X_QcmfM2dYCwS2H5nACFPPaf1BW9VVN919DdvUGk62YHWA7wNxA5gUHisv7fVTRvJ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
