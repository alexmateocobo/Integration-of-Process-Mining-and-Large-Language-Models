{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "This script uses libraries for cloud data access (google.cloud.bigquery), process mining and visualization (pm4py), environment management (dotenv), data handling (pandas), and language model orchestration (langchain and langchain_openai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importlibraries for cloud data access\n",
    "from google.cloud import bigquery\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for logging\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data handling\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for process mining\n",
    "import pm4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication Setup\n",
    "\n",
    "Before accessing Google Cloud services, a service account must be created in the Google Cloud Console, with the necessary IAM roles (e.g., BigQuery Admin) assigned to it. The service account’s JSON key file is securely stored locally, and its path is set using the GOOGLE_APPLICATION_CREDENTIALS environment variable to enable programmatic authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 11:20:19,738 - INFO - Starting OAuth authentication...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=316641064865-57id3o26obibotvs226jeevisjdujha5.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A51350%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform&state=8Tey3xSSvnwo7WprOFtv0TARLz4LLN&access_type=offline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 11:20:25,781 - INFO - \"GET /?state=8Tey3xSSvnwo7WprOFtv0TARLz4LLN&code=4/0AVMBsJhSGxVbFmpp_ElFyf7G7dbsHlgK4JTq4S6BPwXslj4ZAwdCM9TQ3lZtSnvAy1Veqw&scope=https://www.googleapis.com/auth/cloud-platform HTTP/1.1\" 200 65\n",
      "2025-07-12 11:20:26,145 - INFO - Authentication successful.\n",
      "2025-07-12 11:20:26,146 - INFO - BigQuery client initialized.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "project_id = \"integration-of-pm-and-llms\"\n",
    "client_secret_path = \"/Users/alejandromateocobo/Documents/PythonProjects/Integration_Of_LLMs_And_Process_Mining/keys/client_secret_316641064865-57id3o26obibotvs226jeevisjdujha5.apps.googleusercontent.com.json\"\n",
    "\n",
    "# Authentication flow\n",
    "logging.info(\"Starting OAuth authentication...\")\n",
    "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\n",
    "    client_secret_path,\n",
    "    scopes=SCOPES\n",
    ")\n",
    "credentials = flow.run_local_server(port=0)\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "logging.info(\"Authentication successful.\")\n",
    "logging.info(\"BigQuery client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query BigQuery\n",
    "\n",
    "Google Cloud Platform (GCP) is a suite of cloud computing services that enables scalable storage, processing, and data analysis using Google’s infrastructure. To use GCP for analyzing clinical datasets like MIMIC-III, users must create a Google account, set up a GCP project with billing, enable the necessary APIs, and configure OAuth client authentication to securely access cloud resources. The MIMIC-III database, which contains detailed health records from over 40,000 critical care patients, can be accessed through Google BigQuery for efficient cloud-based analysis, which is the recommended method by the MIT Lab for Computational Physiology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 12:03:45,959 - INFO - Running user-provided query...\n",
      "2025-07-12 12:03:47,160 - INFO - Waiting for query to complete...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID 9da185b4-94aa-4bcd-bc0f-9ec8e1b66e7f successfully executed: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 12:03:48,928 - INFO - Query completed. Retrieved 1616 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT e.*\n",
    "        FROM `integration-of-pm-and-llms.integration_of_pm_and_llms.filtered_eventlog` e\n",
    "        INNER JOIN `physionet-data.mimiciii_clinical.icustays` icu\n",
    "        ON e.icustay_id = icu.icustay_id\n",
    "        WHERE e.icustay_id IN (211555, 290738, 236225, 213113)\n",
    "        AND e.linksto = 'datetimeevents'\n",
    "    \"\"\"\n",
    "\n",
    "# Check if the client is initialized before running the query\n",
    "if not client:\n",
    "    raise Exception(\"BigQuery client not initialized. Run authenticate() first.\")\n",
    "\n",
    "# Run the query\n",
    "logging.info(\"Running user-provided query...\")\n",
    "job = client.query(query)\n",
    "\n",
    "logging.info(\"Waiting for query to complete...\")\n",
    "df = job.to_dataframe(progress_bar_type='tqdm')\n",
    "\n",
    "logging.info(f\"Query completed. Retrieved {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   itemid  subject_id  hadm_id  icustay_id     event_timestamp  \\\n",
      "0  224295       47546   112012      236225 2113-04-18 18:09:00   \n",
      "1  224282       97599   135263      213113 2142-01-11 00:00:00   \n",
      "2  225766       97599   135263      213113 2142-01-01 00:00:00   \n",
      "3  224279       97599   135263      213113 2142-01-20 06:00:00   \n",
      "4  224290       97599   135263      213113 2142-01-17 16:45:00   \n",
      "\n",
      "                               label                 category         linksto  \n",
      "0  Cordis/Introducer Dressing Change  Access Lines - Invasive  datetimeevents  \n",
      "1          Multi Lumen Tubing Change  Access Lines - Invasive  datetimeevents  \n",
      "2              Sheath Insertion Date  Access Lines - Invasive  datetimeevents  \n",
      "3        Multi Lumen Dressing Change  Access Lines - Invasive  datetimeevents  \n",
      "4        Arterial line Tubing Change  Access Lines - Invasive  datetimeevents  \n"
     ]
    }
   ],
   "source": [
    "# Process the dataframe as needed\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Event Log for PM4PY\n",
    "\n",
    "In this step, the dataset is reformatted to match the structure expected by PM4PY, where each event log requires a case identifier, an activity name, and a timestamp. The data is then converted into a PM4PY event log object, which enables process mining algorithms to analyze the sequence of events across cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 12:03:57,004 - INFO - Total cases: 4\n",
      "2025-07-12 12:03:57,009 - INFO - Total activities: 35\n",
      "2025-07-12 12:03:57,018 - INFO - Total events: 1616\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for PM4PY (using icustay_id as case identifier)\n",
    "df_eventlog = df.rename(columns={\n",
    "    \"icustay_id\": \"case:concept:name\",  # Update to use icustay_id as case identifier\n",
    "    \"event_timestamp\": \"time:timestamp\",\n",
    "    \"label\": \"concept:name\"\n",
    "})\n",
    "\n",
    "# Convert the timestamp column to datetime\n",
    "df_eventlog[\"time:timestamp\"] = pd.to_datetime(df_eventlog[\"time:timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Format the dataframe using pm4py.utils.format_dataframe\n",
    "df_eventlog = pm4py.utils.format_dataframe(\n",
    "    df_eventlog,\n",
    "    case_id='case:concept:name',\n",
    "    activity_key='concept:name',\n",
    "    timestamp_key='time:timestamp'\n",
    ")\n",
    "\n",
    "# Convert the dataframe to an event log object\n",
    "event_log = pm4py.convert_to_event_log(df_eventlog)\n",
    "\n",
    "# Print basic statistics\n",
    "logging.info(f\"Total cases: {len(set(df_eventlog['case:concept:name']))}\")\n",
    "logging.info(f\"Total activities: {len(set(df_eventlog['concept:name']))}\")\n",
    "logging.info(f\"Total events: {len(df_eventlog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   itemid  subject_id  hadm_id case:concept:name            time:timestamp  \\\n",
      "0  226515       53232   115492            211555 2128-01-02 00:00:00+00:00   \n",
      "1  224290       53232   115492            211555 2151-06-29 00:00:00+00:00   \n",
      "2  224298       53232   115492            211555 2151-06-29 00:00:00+00:00   \n",
      "3  224288       53232   115492            211555 2151-06-29 00:00:00+00:00   \n",
      "4  224298       53232   115492            211555 2151-06-29 00:00:00+00:00   \n",
      "\n",
      "                      concept:name                 category         linksto  \\\n",
      "0                    Date of Birth                      ADT  datetimeevents   \n",
      "1      Arterial line Tubing Change  Access Lines - Invasive  datetimeevents   \n",
      "2  Cordis/Introducer Tubing Change  Access Lines - Invasive  datetimeevents   \n",
      "3     Arterial line Insertion Date  Access Lines - Invasive  datetimeevents   \n",
      "4  Cordis/Introducer Tubing Change  Access Lines - Invasive  datetimeevents   \n",
      "\n",
      "   @@index  @@case_index  \n",
      "0        0             0  \n",
      "1        1             0  \n",
      "2        2             0  \n",
      "3        3             0  \n",
      "4        4             0  \n"
     ]
    }
   ],
   "source": [
    "# Preview the formatted event log\n",
    "print(df_eventlog.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discover and Visualize the Directly-Follows Graph (DFG) with PM4PY\n",
    "\n",
    "In this step, the frequency-based Directly-Follows Graph (DFG) is discovered from the event log using PM4PY and visualized to understand the control-flow structure of the process. The DFG is then converted into a readable textual format to prepare it for further analysis with a Large Language Model (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover the Directly-Follows Graph (DFG)\n",
    "dfg, start_activities, end_activities = pm4py.discovery.discover_dfg(\n",
    "    event_log,\n",
    "    activity_key='concept:name',\n",
    "    case_id_key='case:concept:name',\n",
    "    timestamp_key='time:timestamp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the DFG\n",
    "pm4py.vis.view_dfg(\n",
    "    dfg,\n",
    "    start_activities,\n",
    "    end_activities,\n",
    "    format='png',       # You can also use 'svg' if supported\n",
    "    bgcolor='white',\n",
    "    rankdir='LR'        # Left-to-right layout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover Petri net using the Inductive Miner\n",
    "net, im, fm = pm4py.discovery.discover_petri_net_inductive(\n",
    "    event_log,\n",
    "    activity_key='concept:name',\n",
    "    case_id_key='case:concept:name',\n",
    "    timestamp_key='time:timestamp',\n",
    "    multi_processing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Petri net\n",
    "pm4py.vis.view_petri_net(net, im, fm, format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
